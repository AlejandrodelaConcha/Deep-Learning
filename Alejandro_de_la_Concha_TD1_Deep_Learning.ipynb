{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alejandro de la Concha TD1 Deep Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejandrodelaConcha/Deep-Learning/blob/master/Alejandro_de_la_Concha_TD1_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wrUIVcz_4htq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "path=os.path.dirname(os.getcwd())\n",
        "sys.path.append(path)\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "import copy\n",
        "# On some implementations of matplotlib, you may need to change this value\n",
        "IMAGE_SIZE = 72\n",
        "\n",
        "def generate_a_drawing(figsize, U, V, noise=0.0):\n",
        "    fig = plt.figure(figsize=(figsize,figsize))\n",
        "    ax = plt.subplot(111)\n",
        "    plt.axis('Off')\n",
        "    ax.set_xlim(0,figsize)\n",
        "    ax.set_ylim(0,figsize)\n",
        "    ax.fill(U, V, \"k\")\n",
        "    fig.canvas.draw()\n",
        "    imdata = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)[::3].astype(np.float32)\n",
        "    imdata = imdata + noise * np.random.random(imdata.size)\n",
        "    plt.close(fig)\n",
        "    return imdata\n",
        "\n",
        "def generate_a_rectangle(noise=0.0, free_location=False):\n",
        "    figsize = 1.0    \n",
        "    U = np.zeros(4)\n",
        "    V = np.zeros(4)\n",
        "    if free_location:\n",
        "        corners = np.random.random(4)\n",
        "        top = max(corners[0], corners[1])\n",
        "        bottom = min(corners[0], corners[1])\n",
        "        left = min(corners[2], corners[3])\n",
        "        right = max(corners[2], corners[3])\n",
        "    else:\n",
        "        side = (0.3 + 0.7 * np.random.random()) * figsize\n",
        "        top = figsize/2 + side/2\n",
        "        bottom = figsize/2 - side/2\n",
        "        left = bottom\n",
        "        right = top\n",
        "    U[0] = U[1] = top\n",
        "    U[2] = U[3] = bottom\n",
        "    V[0] = V[3] = left\n",
        "    V[1] = V[2] = right\n",
        "    return generate_a_drawing(figsize, U, V, noise)\n",
        "\n",
        "def generate_rectangles(noise=0.0, free_location=False):\n",
        "    figsize = 1.0    \n",
        "    U = np.zeros(4)\n",
        "    V = np.zeros(4)\n",
        "    if free_location:\n",
        "        corners = np.random.random(4)\n",
        "        top = max(corners[0], corners[1])\n",
        "        bottom = min(corners[0], corners[1])\n",
        "        left = min(corners[2], corners[3])\n",
        "        right = max(corners[2], corners[3])\n",
        "    else:\n",
        "        side = (0.3 + 0.7 * np.random.random()) * figsize\n",
        "        top = figsize/2 + side/2\n",
        "        bottom = figsize/2 - side/2\n",
        "        left = bottom\n",
        "        right = top\n",
        "    U[0] = U[1] = top\n",
        "    U[2] = U[3] = bottom\n",
        "    V[0] = V[3] = left\n",
        "    V[1] = V[2] = right\n",
        "    return [generate_a_drawing(figsize, U, V,noise),generate_a_drawing(figsize, U, V,0)]\n",
        "\n",
        "\n",
        "def generate_a_disk(noise=0.0, free_location=False):\n",
        "    figsize = 1.0\n",
        "    if free_location:\n",
        "        center = np.random.random(2)\n",
        "    else:\n",
        "        center = (figsize/2, figsize/2)\n",
        "    radius = (0.3 + 0.7 * np.random.random()) * figsize/2\n",
        "    N = 50\n",
        "    U = np.zeros(N)\n",
        "    V = np.zeros(N)\n",
        "    i = 0\n",
        "    for t in np.linspace(0, 2*np.pi, N):\n",
        "        U[i] = center[0] + np.cos(t) * radius\n",
        "        V[i] = center[1] + np.sin(t) * radius\n",
        "        i = i + 1\n",
        "    return generate_a_drawing(figsize, U, V, noise)\n",
        "\n",
        "def generate_disks(noise=0.0, free_location=False):\n",
        "    figsize = 1.0\n",
        "    if free_location:\n",
        "        center = np.random.random(2)\n",
        "    else:\n",
        "        center = (figsize/2, figsize/2)\n",
        "    radius = (0.3 + 0.7 * np.random.random()) * figsize/2\n",
        "    N = 50\n",
        "    U = np.zeros(N)\n",
        "    V = np.zeros(N)\n",
        "    i = 0\n",
        "    for t in np.linspace(0, 2*np.pi, N):\n",
        "        U[i] = center[0] + np.cos(t) * radius\n",
        "        V[i] = center[1] + np.sin(t) * radius\n",
        "        i = i + 1\n",
        "    return [generate_a_drawing(figsize, U, V,noise),generate_a_drawing(figsize, U, V,0)]\n",
        "\n",
        "def generate_a_triangle(noise=0.0, free_location=False):\n",
        "    figsize = 1.0\n",
        "    if free_location:\n",
        "        U = np.random.random(3)\n",
        "        V = np.random.random(3)\n",
        "    else:\n",
        "        size = (0.3 + 0.7 * np.random.random())*figsize/2\n",
        "        middle = figsize/2\n",
        "        U = (middle, middle+size, middle-size)\n",
        "        V = (middle+size, middle-size, middle-size)\n",
        "    imdata = generate_a_drawing(figsize, U, V, noise)\n",
        "    return [imdata, [U[0], V[0], U[1], V[1], U[2], V[2]]]\n",
        "\n",
        "def generate_triangles(noise=0.0, free_location=False):\n",
        "    figsize = 1.0\n",
        "    if free_location:\n",
        "        U = np.random.random(3)\n",
        "        V = np.random.random(3)\n",
        "    else:\n",
        "        size = (0.3 + 0.7 * np.random.random())*figsize/2\n",
        "        middle = figsize/2\n",
        "        U = (middle, middle+size, middle-size)\n",
        "        V = (middle+size, middle-size, middle-size)\n",
        "    imdata= generate_a_drawing(figsize, U, V, 0)\n",
        "    imdata_noisy= generate_a_drawing(figsize, U, V, noise)\n",
        "    return [imdata_noisy,imdata,  [U[0], V[0], U[1], V[1], U[2], V[2]]]\n",
        "\n",
        "\n",
        "#im = generate_a_rectangle(10, True)\n",
        "#plt.imshow(im.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "\n",
        "#im = generate_a_disk(10)\n",
        "#plt.imshow(im.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "\n",
        "#[im, v] = generate_a_triangle(20, False)\n",
        "#plt.imshow(im.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "\n",
        "\n",
        "def generate_dataset_classification(nb_samples, noise=0.0, free_location=False,verbose=True):\n",
        "    # Getting im_size:\n",
        "    im_size = generate_a_rectangle().shape[0]\n",
        "    X = np.zeros([nb_samples,im_size])\n",
        "    Y = np.zeros(nb_samples)\n",
        "    print('Creating data:')\n",
        "    for i in range(nb_samples):\n",
        "        if i % 10 == 0:\n",
        "            if(verbose):\n",
        "                print(i)\n",
        "        category = np.random.randint(3)\n",
        "        if category == 0:\n",
        "            X[i] = generate_a_rectangle(noise, free_location)\n",
        "        elif category == 1: \n",
        "            X[i] = generate_a_disk(noise, free_location)\n",
        "        else:\n",
        "            [X[i], V] = generate_a_triangle(noise, free_location)\n",
        "        Y[i] = category\n",
        "    X = (X + noise) / (255 + 2 * noise)\n",
        "    return [X, Y]\n",
        "\n",
        "\n",
        "def generate_test_set_classification(free_location=False,verbose=True):\n",
        "    np.random.seed(42)\n",
        "    [X_test, Y_test] = generate_dataset_classification(300, 20, free_location,verbose)\n",
        "    Y_test = np_utils.to_categorical(Y_test, 3) \n",
        "    return [X_test, Y_test]\n",
        "\n",
        "def generate_dataset_regression(nb_samples, noise=0.0,free_location=True,verbose=True):\n",
        "    # Getting im_size:\n",
        "    im_size = generate_a_triangle()[0].shape[0]\n",
        "    X = np.zeros([nb_samples,im_size])\n",
        "    Y = np.zeros([nb_samples, 6])\n",
        "    print('Creating data:')\n",
        "    for i in range(nb_samples):\n",
        "        if i % 10 == 0:\n",
        "            if verbose:\n",
        "                print(i)\n",
        "        [X[i], Y[i]] = generate_a_triangle(noise, free_location)\n",
        "    X = (X + noise) / (255 + 2 * noise)\n",
        "    return [X, Y]\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def visualize_prediction(x, y):\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    I = x.reshape((IMAGE_SIZE,IMAGE_SIZE))\n",
        "    ax.imshow(I, extent=[-0.15,1.15,-0.15,1.15],cmap='gray')\n",
        "    ax.set_xlim([0,1])\n",
        "    ax.set_ylim([0,1])\n",
        "\n",
        "    xy = y.reshape(3,2)\n",
        "    tri = patches.Polygon(xy, closed=True, fill = False, edgecolor = 'r', linewidth = 5, alpha = 0.5)\n",
        "    ax.add_patch(tri)\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def generate_test_set_regression(free_location=True,verbose=False):\n",
        "    np.random.seed(42)\n",
        "    [X_test, Y_test] = generate_dataset_regression(300, 20,free_location,verbose=False)\n",
        "    return [X_test, Y_test]\n",
        "\n",
        "\n",
        "def generate_dataset_denoising(nb_samples, noise=0.0, free_location=False,verbose=True):\n",
        "    # Getting im_size:\n",
        "    im_size = generate_a_rectangle().shape[0]\n",
        "    X = np.zeros([nb_samples,im_size])\n",
        "    Y = np.zeros([nb_samples,im_size])\n",
        "    print('Creating data:')\n",
        "    for i in range(nb_samples):\n",
        "        if i % 10 == 0:\n",
        "            if verbose:\n",
        "                print(i)\n",
        "        category = np.random.randint(3)\n",
        "        if category == 0:\n",
        "            [X[i],Y[i]] = generate_rectangles(noise, free_location)\n",
        "        elif category == 1: \n",
        "            [X[i],Y[i]]= generate_disks(noise, free_location)\n",
        "        else:\n",
        "            [X[i],Y[i], V] = generate_triangles(noise, free_location)\n",
        "    \n",
        "    X = (X  + noise)/ (255 + 2 * noise)    \n",
        "    Y = (Y) / (255)\n",
        "    return [X, Y]\n",
        "\n",
        "def generate_test_set_denoising(verbose=True):\n",
        "    np.random.seed(42)\n",
        "    [X_test, Y_test] = generate_dataset_denoising(300, 20,True,verbose)\n",
        "    return [X_test, Y_test]\n",
        "\n",
        "def compute_angles(Y):\n",
        "    Y=Y+1\n",
        "    first_vector_1=Y[[2,3]]-Y[[0,1]]\n",
        "    second_vector_1=Y[[4,5]]-Y[[0,1]]\n",
        "    cos_1= first_vector_1.dot(second_vector_1)/(np.sqrt(np.dot(first_vector_1,first_vector_1))*np.sqrt(np.dot(second_vector_1,second_vector_1)))\n",
        "    theta_1=np.arccos(cos_1)\n",
        "\n",
        "    first_vector_2=Y[[0,1]]-Y[[2,3]]\n",
        "    second_vector_2=Y[[4,5]]-Y[[2,3]]\n",
        "    cos_2= first_vector_2.dot(second_vector_2)/(np.sqrt(np.dot(first_vector_2,first_vector_2))*np.sqrt(np.dot(second_vector_2,second_vector_2)))\n",
        "    theta_2=np.arccos(cos_2)\n",
        "\n",
        "    first_vector_3=Y[[0,1]]-Y[[4,5]]\n",
        "    second_vector_3=Y[[2,3]]-Y[[4,5]]\n",
        "    cos_3= first_vector_3.dot(second_vector_3)/(np.sqrt(np.dot(first_vector_3,first_vector_3))*np.sqrt(np.dot(second_vector_3,second_vector_3)))\n",
        "    theta_3=np.arccos(cos_3)\n",
        "\n",
        "    return(np.array([theta_1,theta_2,theta_3]))\n",
        "    \n",
        "def order_by_angles(Y):\n",
        "    Y_ordered=copy.deepcopy(Y)\n",
        "    if(len(Y.shape)>1):\n",
        "        for i in range(Y.shape[0]):\n",
        "            thetas=compute_angles(Y[i])\n",
        "            first_point=np.argsort(thetas)\n",
        "            Y_ordered[i][[0,2,4]]=Y[i][[0,2,4]][first_point]\n",
        "            Y_ordered[i][[1,3,5]]=Y[i][[1,3,5]][first_point]\n",
        "    else:\n",
        "        thetas=compute_angles(Y)\n",
        "        first_point=np.argsort(thetas)\n",
        "        Y_ordered[[0,2,4]]=Y[[0,2,4]][first_point]\n",
        "        Y_ordered[[1,3,5]]=Y[[1,3,5]][first_point]\n",
        "    return(Y_ordered)\n",
        "    \n",
        "\n",
        "\n",
        "from keras.models import Sequential,Model,load_model\n",
        "from keras.layers import Dense,Dropout,Input,Flatten,Conv2D, MaxPooling2D,UpSampling2D,merge,concatenate,BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers,layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-3nCSWb4ht5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Classification"
      ]
    },
    {
      "metadata": {
        "id": "sd-yIiWb4ht8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I generated a dataset with 300 observtions."
      ]
    },
    {
      "metadata": {
        "id": "qouiHHc04ht-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_train, Y_train] = generate_dataset_classification(300, 20,verbose=False) ### I add verbose to not see the number of points\n",
        "Y_train=np_utils.to_categorical(Y_train)\n",
        "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QIQj1cWQ4huE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the first task is classification, the problem suggests to create a linear classifier that will be equivalent to just have a layer representing the three classes with softmax as activation function. The loss function used in the problem was cross entropy. \n",
        "\n",
        "I trained 3 different models with different optimizers. The goal is to compare ADAM vs the Stochastic Gradient Descent (SGD) as indicated in the HW. The three models were trained by using 100 epochs. For the ADAM model I used a batch size of 32, for the SG a learning rate of 0.1 and 0.001. Aditionally, I analyzed the modified SGD with a momentum of 0.9. "
      ]
    },
    {
      "metadata": {
        "id": "G3RJkwVI4huF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################# ADAM\n",
        "\n",
        "model_ADAM=Sequential()\n",
        "model_ADAM.add(Dense(3,activation=\"softmax\",input_shape=(X_train.shape[1],)))\n",
        "model_ADAM.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "evolution_ADAM=model_ADAM.fit(X_train,Y_train,\n",
        "          batch_size=32,epochs=100,verbose=0)\n",
        "\n",
        "################# SGD 1 with lr=0.01\n",
        "\n",
        "model_SGD_1=Sequential()\n",
        "model_SGD_1.add(Dense(3,activation=\"softmax\",input_shape=(X_train.shape[1],)))\n",
        "sgd = optimizers.SGD()\n",
        "model_SGD_1.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=sgd,\n",
        "              metrics=[\"accuracy\"])\n",
        "evolution_SGD_1=model_SGD_1.fit(X_train,Y_train,epochs=100,verbose=0)\n",
        "\n",
        "################# SGD with lr=0.001\n",
        "\n",
        "model_SGD_2=Sequential()\n",
        "model_SGD_2.add(Dense(3,activation=\"softmax\",input_shape=(X_train.shape[1],)))\n",
        "sgd = optimizers.SGD(lr=0.001)\n",
        "model_SGD_2.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=sgd,\n",
        "              metrics=[\"accuracy\"])\n",
        "evolution_SGD_2=model_SGD_2.fit(X_train,Y_train,epochs=100,verbose=0)\n",
        "\n",
        "################# SGD 3 with lr=0.001 and momentum=0.9\n",
        "\n",
        "model_SGD_3=Sequential()\n",
        "model_SGD_3.add(Dense(3,activation=\"softmax\",input_shape=(X_train.shape[1],)))\n",
        "sgd = optimizers.SGD(lr=0.001,momentum=0.9)\n",
        "model_SGD_3.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=sgd,\n",
        "              metrics=[\"accuracy\"])\n",
        "evolution_SGD_3=model_SGD_3.fit(X_train,Y_train,epochs=100,verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUNDPOzl4huK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I generate the test set to compare the models. "
      ]
    },
    {
      "metadata": {
        "id": "KgRzOtWt4huL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_test, Y_test]=generate_test_set_classification(verbose=False)\n",
        "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJDghZi44huP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see in the figure bellow how the ADAM optimizer converged faster compared with the SGD without momentum. In fact, in practice, the ADAM optimizer is commonly preferred.\n",
        "Additionally, we can see the sensitivity of the SGD to the learning rate. A bad value of the learning rate can lead to poor results as the orange line indicates.\n",
        "In this example, we can see how adding the momentum parameter can improve a lot the performance of the SGD optimizer by allowing it to scape from saddle points. \n",
        "As we saw in the lectures, in general, the choice of the optimizer depends more on our knowledge of how to modify the parameters."
      ]
    },
    {
      "metadata": {
        "id": "jkBPuoQ84huP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "plt.plot(evolution_ADAM.history['acc'])\n",
        "plt.plot(evolution_SGD_1.history['acc'])\n",
        "plt.plot(evolution_SGD_2.history['acc'])\n",
        "plt.plot(evolution_SGD_3.history['acc'])\n",
        "plt.legend(['Adam', 'SGD with lr=0.01',\"SGD with lr=0.001\",\"SGD with lr=0.001 and momentum=0.9\"], loc='upper left')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SrSDZE934huY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we valuate the models in a test set, I found an accuracy of $99.6%$ for the SGD with momentum. The ADAM optimizer gets a smaller value for the loss function an accuracy of $100\\%$. In this example, I would choose the ADMA"
      ]
    },
    {
      "metadata": {
        "id": "uFruTpr24hua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"ADAM \"+str(model_ADAM.evaluate(X_test,Y_test,verbose=0)))\n",
        "print('SGD with lr=0.01 '+str(model_SGD_1.evaluate(X_test,Y_test,verbose=0)))\n",
        "print(\"SGD with lr=0.001 \"+str(model_SGD_2.evaluate(X_test,Y_test,verbose=0)))\n",
        "print(\"SGD with lr=0.001 and momentum=0.9 \"+str(model_SGD_3.evaluate(X_test,Y_test,verbose=0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZgT32UU4hue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualization of the solution"
      ]
    },
    {
      "metadata": {
        "id": "Ysahu5Lw4huf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The figures below show the weights of the links that go from each pixel to each category. The categories are rectangle, disk and triangle, in that order. I compared the ADAM and the SGD with momentum models. The darker the pixer the bigger the value of the weight of the pixel in the soft-max function of the corresponding category. \n",
        "\n",
        "These figures give us an intuition about what features the NN is stracting from the data to classify yhe images.In both models, we can see a rectangle and a triangle detector. For the remainning figures, the results are not clear. It is interesting how, even in a easy task is difficult to interpret what a nn is doing. \n",
        "\n",
        "Nevertheless, in both models, the three figures are different showing that the soft-max functions ponderate differently the pixels accordind to the figure they train to predict."
      ]
    },
    {
      "metadata": {
        "id": "8qesB0VF4hug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE=72\n",
        "cl0=model_SGD_3.get_weights()[0][:,0]\n",
        "cl1=model_SGD_3.get_weights()[0][:,1]\n",
        "cl2=model_SGD_3.get_weights()[0][:,2]\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(cl0.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(cl1.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(cl2.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cC0a6WF84huo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE=72\n",
        "cl0=model_ADAM.get_weights()[0][:,0]\n",
        "cl1=model_ADAM.get_weights()[0][:,1]\n",
        "cl2=model_ADAM.get_weights()[0][:,2]\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(cl0.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(cl1.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(cl2.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUgAfsRH4huw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A more difficult classification problem."
      ]
    },
    {
      "metadata": {
        "id": "9JOTkoTh4huy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, once again, I tried to predict the cluster of each observation, but this time the position and orientation of the figures change. I tried multiple models with relu and softmax as activation functions. In all cases I use the crossentropy as loss function.I show here the results for the model suggested in the HW without any regulization and another model with regularization and an extra fully connected network."
      ]
    },
    {
      "metadata": {
        "id": "BthMITY34huy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I generated 1000 samples of the train set as I see that the models had problems to generalize with only 300."
      ]
    },
    {
      "metadata": {
        "id": "-a1LJzhE4huz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_train, Y_train] = generate_dataset_classification(1000, 20, True, verbose=False)\n",
        "X_train=X_train.reshape(X_train.shape[0],72,72,1)\n",
        "Y_train=np_utils.to_categorical(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PP58v_NP4hu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Suggested model in the HW: 1 convolutional layer with 16 5x5 flters, 1 pooling layer, and one fully connected layer."
      ]
    },
    {
      "metadata": {
        "id": "vg8Zrz9A4hu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_suggested=Sequential()\n",
        "model_suggested.add(Conv2D(16,(5,5),activation=\"relu\",input_shape=(72,72,1)))\n",
        "model_suggested.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model_suggested.add(Flatten())\n",
        "model_suggested.add(Dense(3,activation=\"softmax\"))\n",
        "model_suggested.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model_suggested.fit(X_train,Y_train,\n",
        "          batch_size=32,epochs=100,verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5wTkAJL24hvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model that improves results (model 2) : Suggested model in the HW: 1 convolutional layer with 16 5x5 flters, 1 pooling layer, and two fully connected layer and with dropout to avoid overfiting   "
      ]
    },
    {
      "metadata": {
        "id": "5XSjXB_A4hvF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_2=Sequential()\n",
        "model_2.add(Conv2D(16,(5,5),activation=\"relu\",input_shape=(72,72,1)))\n",
        "model_2.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model_2.add(Dropout(0.25))\n",
        "model_2.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model_2.add(Dropout(0.25))\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(128,activation=\"relu\"))\n",
        "model_2.add(Dropout(0.5))\n",
        "model_2.add(Dense(3,activation=\"softmax\"))\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model_2.fit(X_train,Y_train,\n",
        "          batch_size=32,epochs=100,verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HCVWAn2I4hvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I generated a test set to compare both models."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "axqMqAHU4hvR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_test, Y_test]=generate_test_set_classification(True,verbose=False)\n",
        "X_test=X_test.reshape(X_test.shape[0],72,72,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ydgo5H344hvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After training the suggested model, I realized that the difference between the accuracy in the train and test set was big showing overfitting. \n",
        "A way to avoid this is to use regularization, for that reason in model 2 I add dropout, that is in the training , the algorithm eliminate some paths with a given probability. I also added a layer. This improved considerable the model as we can see: going from an accuracy of $87\\%$ to an accuracy of $95\\%$ in the test set. We can see how the gap between the accuracies in model 2 is smaller showing a smaller degree of overfitting. "
      ]
    },
    {
      "metadata": {
        "id": "BOPABxpI4hvX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Model suggested in train set \"+str(model_suggested.evaluate(X_train,Y_train,verbose=2)))\n",
        "print(\"Model suggested in test set \"+str(model_suggested.evaluate(X_test,Y_test,verbose=2)))\n",
        "print(\"Model 2 in train set \"+str(model_2.evaluate(X_train,Y_train,verbose=2)))\n",
        "print(\"Model 2 in test set \"+str(model_2.evaluate(X_test,Y_test,verbose=2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QniMcbm4hvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A regression problem."
      ]
    },
    {
      "metadata": {
        "id": "K1P-WrGM7O-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, I trained different CNN to detect the vertices of a triangle. As this problem is harder than a classification problem,  I increased the number of samples to 5000. \n",
        "I started by doing a normalization of the train set. That is, I reordered the set of vertices according to their x value. \n",
        "\n",
        "I also tried to order the vertices according to the angles of the triangle. But I was not satisfied with the results. \n",
        "\n",
        "I got the best results by ordering by the x value.\n"
      ]
    },
    {
      "metadata": {
        "id": "eL9jajfx4hva",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def order_corners(Y):\n",
        "    Y_ordered=Y_train\n",
        "    for i in range(Y.shape[0]):\n",
        "        first_point=np.argsort([Y[i][0],Y[i][2],Y[i][4]])\n",
        "        Y_ordered[i][[0,2,4]]=Y_train[i][[0,2,4]][first_point]\n",
        "        Y_ordered[i][[1,3,5]]=Y_train[i][[1,3,5]][first_point]\n",
        "    return(Y_ordered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bh9JTRge4hvd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_train, Y_train] = generate_dataset_regression(5000, 20,verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NL2xbnai4hvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train=X_train.reshape(X_train.shape[0],72,72,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEwgbgfy4hvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_train_ordered=order_corners(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OVqeQweM7h0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first model I analyzed was the suggested one. The results are far away of being optimal. The problem requiere us to learn features that depend on the position and orientation of the triangle and just using a convolutional network is enable to do that."
      ]
    },
    {
      "metadata": {
        "id": "NBwh8yz64hvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mcp = ModelCheckpoint('suggested_reg_model.hdf5', monitor=\"val_loss\",\n",
        "                      save_best_only=True, save_weights_only=False)\n",
        "model_suggested_reg=Sequential()\n",
        "model_suggested_reg.add(Conv2D(16,(5,5),activation=\"relu\",input_shape=(72,72,1)))\n",
        "model_suggested_reg.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model_suggested_reg.add(Flatten())\n",
        "model_suggested_reg.add(Dense(6))\n",
        "model_suggested_reg.compile(loss=\"mean_squared_error\",\n",
        "              optimizer=\"adam\")\n",
        "model_suggested_reg.fit(X_train,Y_train_ordered,\n",
        "          batch_size=32,validation_split=0.1,epochs=300,verbose=2,callbacks=[mcp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ucsb9Ix54hvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_test, Y_test] = generate_test_set_regression(verbose=False)\n",
        "X_test=X_test.reshape(X_test.shape[0],72,72,1)\n",
        "Y_test_ordered=order_corners(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xT8aPorC4hv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_predict_1=model_suggested_reg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8FbpCS934hv_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    visualize_prediction(X_test[i], Y_predict_1[i])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EeyzN1wxC25J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the secon model,I increased the number of convulutional layers, a dense layer and I added dropout to avoid overfitting. The results are better, but they can be highly improved. I tried many differet combinations but I did not get the results I wanted."
      ]
    },
    {
      "metadata": {
        "id": "Fvf0tqm-4hwF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_2_reg=Sequential()\n",
        "model_2_reg.add(Conv2D(32,(5,5),activation=\"relu\",input_shape=(72,72,1)))\n",
        "model_2_reg.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_2_reg.add(Dropout(0.5))\n",
        "model_2_reg.add(Conv2D(32,(5,5),activation=\"relu\",input_shape=(72,72,1)))\n",
        "model_2_reg.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_2_reg.add(Dropout(0.25))\n",
        "model_2_reg.add(Conv2D(32,(5,5),activation=\"relu\",input_shape=(72,72,1)))\n",
        "model_2_reg.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_2_reg.add(Flatten())\n",
        "model_2_reg.add(Dropout(0.25))\n",
        "model_2_reg.add(Dense(64,activation=\"relu\"))\n",
        "model_2_reg.add(Dense(6))\n",
        "adam = optimizers.adam(lr=1e-4)\n",
        "model_2_reg.compile(loss=\"mean_squared_error\",optimizer=adam)\n",
        "model_2_reg.fit(X_train,Y_train_ordered,\n",
        "          batch_size=32,validation_split=0.1,epochs=250,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-dhPco54hwH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_predict_2=model_2_reg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-jtXR4V4hwK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(40,50):\n",
        "    visualize_prediction(X_test[i], Y_predict_2[i])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1cBex5UiLYKn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qKjLKQqZ4hwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image denoising "
      ]
    },
    {
      "metadata": {
        "id": "XHH0lfzP4hwV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I generated a train set with 500 samples."
      ]
    },
    {
      "metadata": {
        "id": "0LSXHoSz4hwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_train,Y_train] = generate_dataset_denoising(500, 20,True,verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmBEhJ9w4hwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train=X_train.reshape(X_train.shape[0],72,72,1)\n",
        "Y_train=Y_train.reshape(Y_train.shape[0],72,72,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I8l7blsH4hwa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I trained different hourglass networks. Here you can see the one that I could train faster and had a good performance. Normally, this kind of architecture is used in image segmentation, but in this case I tried to predict the pixel intensity by doing a regression task. For that reason, I did experiments with both loss functions , \"mean_squared_error\" and \"mean_absolute_error\". I preferred the results given by the \"mean_squared_error\". \n",
        "\n",
        "In a normal hourglass architecture, batchnormalization is done after convolutional layers in order to guarantee the stability of the network during the training and it reduces overfitting. Nevetheless, in this particular case, it just made the training process slower and it did not improve the results, so I did not include it. \n",
        "\n",
        "The trained hourglas networks use Pooling and Upsampling of dimension 2x2."
      ]
    },
    {
      "metadata": {
        "id": "3NT_vvVw4hwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mcp = ModelCheckpoint('denoising_model_1.hdf5', monitor=\"val_loss\",\n",
        "                      save_best_only=True, save_weights_only=False)\n",
        "\n",
        "\n",
        "neural_input=Input((72,72,1))\n",
        "\n",
        "conv1=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(neural_input)\n",
        "#conv1=BatchNormalization()(conv1)\n",
        "conv1=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(conv1)\n",
        "#conv1=BatchNormalization()(conv1)\n",
        "pool1=MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "\n",
        "conv2=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(pool1)\n",
        "#conv2=BatchNormalization()(conv2)\n",
        "conv2=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(conv2)\n",
        "#conv2=BatchNormalization()(conv2)\n",
        "pool2=MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "\n",
        "conv3=Conv2D(40,(5,5),activation=\"relu\",padding=\"same\")(pool2)\n",
        "#conv3=BatchNormalization()(conv3)\n",
        "conv3=Conv2D(40,(5,5),activation=\"relu\",padding=\"same\")(conv3)\n",
        "#conv3=BatchNormalization()(conv3)\n",
        "\n",
        "up4=UpSampling2D(size=(2,2))(conv3)\n",
        "\n",
        "up_conv4=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(up4)\n",
        "#up_conv4=BatchNormalization()(up_conv4)\n",
        "merge4 = concatenate([conv2,up_conv4],axis=3)\n",
        "conv4=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(merge4)\n",
        "#conv4=BatchNormalization()(conv4)\n",
        "\n",
        "up5=UpSampling2D(size=(2,2))(conv4)\n",
        "up_conv5=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(up5)\n",
        "#up_conv5=BatchNormalization()(up_conv5)\n",
        "merge5 = concatenate([conv1,up_conv5],axis=3)\n",
        "conv5=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(merge5)\n",
        "#conv5=BatchNormalization()(conv5)\n",
        "\n",
        "conv6 = Conv2D(1,(1,1),activation=\"relu\")(conv5)\n",
        "#conv6=BatchNormalization()(conv6)\n",
        "\n",
        "model_1=Model(inputs=neural_input,outputs=conv6)\n",
        "model_1.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
        "\n",
        "model_1.fit(X_train,Y_train,validation_split=0.1,\n",
        "          batch_size=100,epochs=50,verbose=1,callbacks=[mcp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmWT3pTP4hwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can access to the training model using this line."
      ]
    },
    {
      "metadata": {
        "id": "z4QO9j974hwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[X_test, Y_test] = generate_test_set_denoising(verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSm9nniE4hwv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test=X_test.reshape(X_test.shape[0],72,72,1)\n",
        "Y_test=Y_test.reshape(Y_test.shape[0],72,72,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVuZyDv94hwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This architecture gets  values of 8e-4 of the mean_squared_loss function in the test set and we can see in the fictures how it eliminates noise from the images efficiently. "
      ]
    },
    {
      "metadata": {
        "id": "Ll1u9XWL4hw1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_1.evaluate(X_test,Y_test,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M05x4S0C4hw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_predict_1=model_1.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_uhQre6t4hw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(10,30))\n",
        "for i in range(5):\n",
        "    plt.subplot(5, 3,3*i+1)\n",
        "    plt.imshow(X_test[i].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "    plt.subplot(5, 3,3*i+2)\n",
        "    plt.imshow(Y_predict_1[i].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "    plt.subplot(5, 3,3*i+3)\n",
        "    plt.imshow(Y_test[i].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9tDpKSY4hxH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In a second model, I tried to predict whether the pixel was black of white and I performed image segmentation based on the previous hourglass architecture, without Batchnormalization, but in this case I added an adittional block of convolutional networks with 2 convolutional networks with 80 filters. I also changed the concatenation order. Now, I concatenated each Upsampling with the previous convolutional network of the same dimension.  As the approach changed, the final network returns the probability of being white or black and the loss function is the binary crosss entropy."
      ]
    },
    {
      "metadata": {
        "id": "bA6X41oQ4hxH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mcp = ModelCheckpoint('denoising_model_2.hdf5', monitor=\"val_loss\",\n",
        "                      save_best_only=True, save_weights_only=False)\n",
        "\n",
        "neural_input=Input((72,72,1))\n",
        "\n",
        "conv1=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(neural_input)\n",
        "conv1=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(conv1)\n",
        "pool1=MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "\n",
        "conv2=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(pool1)\n",
        "conv2=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(conv2)\n",
        "pool2=MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "\n",
        "conv3=Conv2D(40,(5,5),activation=\"relu\",padding=\"same\")(pool2)\n",
        "conv3=Conv2D(40,(5,5),activation=\"relu\",padding=\"same\")(conv3)\n",
        "pool3=MaxPooling2D(pool_size=(2,2))(conv3)\n",
        "\n",
        "conv4=Conv2D(80,(5,5),activation=\"relu\",padding=\"same\")(pool3)\n",
        "conv4=Conv2D(80,(5,5),activation=\"relu\",padding=\"same\")(conv4)\n",
        "\n",
        "up5=UpSampling2D(size=(2,2))(conv4)\n",
        "merge5 = concatenate([conv3,up5],axis=3)\n",
        "\n",
        "conv5=Conv2D(40,(5,5),activation=\"relu\",padding=\"same\")(merge5)\n",
        "conv5=Conv2D(40,(5,5),activation=\"relu\",padding=\"same\")(conv5)\n",
        "\n",
        "up6=UpSampling2D(size=(2,2))(conv5)\n",
        "merge6 = concatenate([conv2,up6],axis=3)\n",
        "\n",
        "conv6=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(merge6)\n",
        "conv6=Conv2D(20,(5,5),activation=\"relu\",padding=\"same\")(conv6)\n",
        "\n",
        "up7=UpSampling2D(size=(2,2))(conv6)\n",
        "merge7 = concatenate([conv1,up7],axis=3)\n",
        "\n",
        "conv7=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(merge7)\n",
        "conv7=Conv2D(10,(5,5),activation=\"relu\",padding=\"same\")(conv7)\n",
        "\n",
        "conv8 = Conv2D(1,(1,1),activation=\"sigmoid\")(conv7)\n",
        "model_cross_entropy=Model(inputs=neural_input,outputs=conv8)\n",
        "model_cross_entropy.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "              optimizer='adam')\n",
        "model_cross_entropy.fit(X_train,Y_train,validation_split=0.1,\n",
        "          batch_size=100,epochs=100,verbose=2,callbacks=[mcp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbyyjctY4hxR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The performance of this architecture is also satisfying, getting an accuracy of  almost  99% in the test set.\n",
        "As mentioned before, the results show the probability of each pixel of being white of black. In this model, the figures are more friendly to the human eye, as the fond is white. "
      ]
    },
    {
      "metadata": {
        "id": "v5Ip6Bx54hxR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_cross_entropy.evaluate(X_test,Y_test,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yZvkCajg4hxV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_predict_2=model_cross_entropy.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47l6HsgY4hxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(10,30))\n",
        "for i in range(5):\n",
        "    plt.subplot(5, 3,3*i+1)\n",
        "    plt.imshow(X_test[i].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "    plt.subplot(5, 3,3*i+2)\n",
        "    plt.imshow(Y_predict_2[i].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
        "    plt.subplot(5, 3,3*i+3)\n",
        "    plt.imshow(Y_test[i].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}